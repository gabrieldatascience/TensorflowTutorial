{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamic Recurrent Neural Network.\n",
    "\n",
    "Implementación de TensorFlow de una Red Neural Recurrente (LSTM) que realiza computación dinámica sobre secuencias de longitud variable. Este ejemplo utiliza un conjunto de datos de juguetes para clasificar secuencias lineales. Las secuencias generadas tienen longitud variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN\n",
    "\n",
    "<img src=\"http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-unrolled.png\" alt=\"nn\" style=\"width: 600px;\"/>\n",
    "\n",
    "Referencias:\n",
    "- [Long Short Term Memory](http://deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf), Sepp Hochreiter & Jurgen Schmidhuber, Neural Computation 9(8): 1735-1780, 1997."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ====================\n",
    "#  GENERADOR DE DATOS\n",
    "# ====================\n",
    "\n",
    "class ToySequenceData(object):\n",
    "    \"\"\" Generar secuencia de datos con longitud dinámica.\n",
    "    Esta clase genera muestras para el entrenamiento:\n",
    "    - Clase 0: secuencias lineales (es decir,[0, 1, 2, 3,....])\n",
    "    - Clase 1: secuencias aleatorias (es decir. [1, 3, 10, 7,...])\n",
    "\n",
    "   AVISO:\n",
    "    Tenemos que rellenar cada secuencia para alcanzar 'max_seq_len' para TensorFlow\n",
    "    consistencia (no podemos alimentar una matriz numérica con inconsistentes\n",
    "    dimensiones). El cálculo dinámico se llevará a cabo gracias a\n",
    "    Seqlen' atributo que registra cada longitud de secuencia real.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_samples=1000, max_seq_len=20, min_seq_len=3,\n",
    "                 max_value=1000):\n",
    "        self.data = []\n",
    "        self.labels = []\n",
    "        self.seqlen = []\n",
    "        for i in range(n_samples):\n",
    "            # Longitud de secuencia aleatoria\n",
    "            len = random.randint(min_seq_len, max_seq_len)\n",
    "            # Monitorización de la longitud de la secuencia para el cálculo dinámico de TensorFlow\n",
    "            self.seqlen.append(len)\n",
    "            # Añadir una secuencia int aleatoria o lineal (50% prob)\n",
    "            if random.random() < .5:\n",
    "                # Generar una secuencia lineal\n",
    "                rand_start = random.randint(0, max_value - len)\n",
    "                s = [[float(i)/max_value] for i in\n",
    "                     range(rand_start, rand_start + len)]\n",
    "                # Secuencia de almohadillas para una consistencia de dimensiones\n",
    "                s += [[0.] for i in range(max_seq_len - len)]\n",
    "                self.data.append(s)\n",
    "                self.labels.append([1., 0.])\n",
    "            else:\n",
    "                # Generar una secuencia aleatoria\n",
    "                s = [[float(random.randint(0, max_value))/max_value]\n",
    "                     for i in range(len)]\n",
    "                # Secuencia de almohadillas para una consistencia de dimensiones\n",
    "                s += [[0.] for i in range(max_seq_len - len)]\n",
    "                self.data.append(s)\n",
    "                self.labels.append([0., 1.])\n",
    "        self.batch_id = 0\n",
    "\n",
    "    def next(self, batch_size):\n",
    "        \"\"\" Return a batch of data. When dataset end is reached, start over.\n",
    "        \"\"\"\n",
    "        if self.batch_id == len(self.data):\n",
    "            self.batch_id = 0\n",
    "        batch_data = (self.data[self.batch_id:min(self.batch_id +\n",
    "                                                  batch_size, len(self.data))])\n",
    "        batch_labels = (self.labels[self.batch_id:min(self.batch_id +\n",
    "                                                  batch_size, len(self.data))])\n",
    "        batch_seqlen = (self.seqlen[self.batch_id:min(self.batch_id +\n",
    "                                                  batch_size, len(self.data))])\n",
    "        self.batch_id = min(self.batch_id + batch_size, len(self.data))\n",
    "        return batch_data, batch_labels, batch_seqlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ==========\n",
    "#   MODELO\n",
    "# ==========\n",
    "\n",
    "# Parametros\n",
    "learning_rate = 0.01\n",
    "training_steps = 10000\n",
    "batch_size = 128\n",
    "display_step = 200\n",
    "\n",
    "# Network Parametros\n",
    "seq_max_len = 20 # Secuencia de longitud máxima\n",
    "n_hidden = 64 # capa oculta número de características\n",
    "n_classes = 2 # secuencia lineal o no\n",
    "\n",
    "trainset = ToySequenceData(n_samples=1000, max_seq_len=seq_max_len)\n",
    "testset = ToySequenceData(n_samples=500, max_seq_len=seq_max_len)\n",
    "\n",
    "# tf Entrada de gráficos\n",
    "x = tf.placeholder(\"float\", [None, seq_max_len, 1])\n",
    "y = tf.placeholder(\"float\", [None, n_classes])\n",
    "# Un placeholder para indicar la longitud de cada secuencia\n",
    "seqlen = tf.placeholder(tf.int32, [None])\n",
    "\n",
    "# Definir pesos\n",
    "weights = {\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden, n_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dynamicRNN(x, seqlen, weights, biases):\n",
    "\n",
    "    # Preparar la forma de los datos para que coincida con los requisitos de la función `rnn\n",
    "    # Forma actual de entrada de datos: (tamaño_de_lote, n_pasos, n_input)\n",
    "    # Forma requerida: ` n_pasos' tensores lista de formas (batch_size, n_input)\n",
    "    \n",
    "    # Desapilado para obtener una lista de tensores de forma'n_pasos' (batch_size, n_input)\n",
    "    x = tf.unstack(x, seq_max_len, 1)\n",
    "\n",
    "    # Define una célula lstm con tensorflow\n",
    "    lstm_cell = tf.contrib.rnn.BasicLSTMCell(n_hidden)\n",
    "\n",
    "    # Obtenga la salida de la celda lstm, siempre y cuando 'sequence_length' se ejecute de forma dinámica\n",
    "    # cálculo.\n",
    "    outputs, states = tf.contrib.rnn.static_rnn(lstm_cell, x, dtype=tf.float32,\n",
    "                                sequence_length=seqlen)\n",
    "\n",
    "    # Cuando realizamos cálculos dinámicos, debemos recuperar el último\n",
    "    # salida calculada dinámicamente, es decir, si la longitud de una secuencia es 10, necesitamos\n",
    "    # para recuperar la décima salida.\n",
    "    # Sin embargo, TensorFlow aún no soporta indexación avanzada, por lo que creamos\n",
    "    # una opción personalizada que para cada muestra en el tamaño de lote, obtener su longitud y\n",
    "    # obtener la salida correspondiente.\n",
    "\n",
    "    #'outputs' es una lista de salidas en cada paso del tiempo, las empaquetamos en un Tensor\n",
    "    # y volver a cambiar la dimensión a[batch_size, n_step, n_input]\n",
    "    outputs = tf.stack(outputs)\n",
    "    outputs = tf.transpose(outputs, [1, 0, 2])\n",
    "\n",
    "    # Hack para construir la indexación y recuperar la salida correcta.\n",
    "    batch_size = tf.shape(outputs)[0]\n",
    "    # Índices de inicio para cada muestra\n",
    "    index = tf.range(0, batch_size) * seq_max_len + (seqlen - 1)\n",
    "    # Indexear\n",
    "    outputs = tf.gather(tf.reshape(outputs, [-1, n_hidden]), index)\n",
    "\n",
    "    # Activación lineal, utilizando las salidas calculadas anteriormente\n",
    "    return tf.matmul(outputs, weights['out']) + biases['out']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aymeric.damien/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/gradients_impl.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "pred = dynamicRNN(x, seqlen, weights, biases)\n",
    "\n",
    "# Definir la pérdida y el optimizador\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Evaluaar modelo\n",
    "correct_pred = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# Inicializar las variables (es decir, asignar su valor por defecto)\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1, Minibatch Loss= 0.864517, Training Accuracy= 0.42188\n",
      "Step 200, Minibatch Loss= 0.686012, Training Accuracy= 0.43269\n",
      "Step 400, Minibatch Loss= 0.682970, Training Accuracy= 0.48077\n",
      "Step 600, Minibatch Loss= 0.679640, Training Accuracy= 0.50962\n",
      "Step 800, Minibatch Loss= 0.675208, Training Accuracy= 0.53846\n",
      "Step 1000, Minibatch Loss= 0.668636, Training Accuracy= 0.56731\n",
      "Step 1200, Minibatch Loss= 0.657525, Training Accuracy= 0.62500\n",
      "Step 1400, Minibatch Loss= 0.635423, Training Accuracy= 0.67308\n",
      "Step 1600, Minibatch Loss= 0.580433, Training Accuracy= 0.75962\n",
      "Step 1800, Minibatch Loss= 0.475599, Training Accuracy= 0.81731\n",
      "Step 2000, Minibatch Loss= 0.434865, Training Accuracy= 0.83654\n",
      "Step 2200, Minibatch Loss= 0.423690, Training Accuracy= 0.85577\n",
      "Step 2400, Minibatch Loss= 0.417472, Training Accuracy= 0.85577\n",
      "Step 2600, Minibatch Loss= 0.412906, Training Accuracy= 0.85577\n",
      "Step 2800, Minibatch Loss= 0.409193, Training Accuracy= 0.85577\n",
      "Step 3000, Minibatch Loss= 0.406035, Training Accuracy= 0.86538\n",
      "Step 3200, Minibatch Loss= 0.403287, Training Accuracy= 0.87500\n",
      "Step 3400, Minibatch Loss= 0.400862, Training Accuracy= 0.87500\n",
      "Step 3600, Minibatch Loss= 0.398704, Training Accuracy= 0.86538\n",
      "Step 3800, Minibatch Loss= 0.396768, Training Accuracy= 0.86538\n",
      "Step 4000, Minibatch Loss= 0.395017, Training Accuracy= 0.86538\n",
      "Step 4200, Minibatch Loss= 0.393422, Training Accuracy= 0.86538\n",
      "Step 4400, Minibatch Loss= 0.391957, Training Accuracy= 0.85577\n",
      "Step 4600, Minibatch Loss= 0.390600, Training Accuracy= 0.85577\n",
      "Step 4800, Minibatch Loss= 0.389334, Training Accuracy= 0.86538\n",
      "Step 5000, Minibatch Loss= 0.388143, Training Accuracy= 0.86538\n",
      "Step 5200, Minibatch Loss= 0.387015, Training Accuracy= 0.86538\n",
      "Step 5400, Minibatch Loss= 0.385940, Training Accuracy= 0.86538\n",
      "Step 5600, Minibatch Loss= 0.384907, Training Accuracy= 0.86538\n",
      "Step 5800, Minibatch Loss= 0.383904, Training Accuracy= 0.85577\n",
      "Step 6000, Minibatch Loss= 0.382921, Training Accuracy= 0.86538\n",
      "Step 6200, Minibatch Loss= 0.381941, Training Accuracy= 0.86538\n",
      "Step 6400, Minibatch Loss= 0.380947, Training Accuracy= 0.86538\n",
      "Step 6600, Minibatch Loss= 0.379912, Training Accuracy= 0.86538\n",
      "Step 6800, Minibatch Loss= 0.378796, Training Accuracy= 0.86538\n",
      "Step 7000, Minibatch Loss= 0.377540, Training Accuracy= 0.86538\n",
      "Step 7200, Minibatch Loss= 0.376041, Training Accuracy= 0.86538\n",
      "Step 7400, Minibatch Loss= 0.374130, Training Accuracy= 0.85577\n",
      "Step 7600, Minibatch Loss= 0.371514, Training Accuracy= 0.85577\n",
      "Step 7800, Minibatch Loss= 0.367723, Training Accuracy= 0.85577\n",
      "Step 8000, Minibatch Loss= 0.362049, Training Accuracy= 0.85577\n",
      "Step 8200, Minibatch Loss= 0.353558, Training Accuracy= 0.85577\n",
      "Step 8400, Minibatch Loss= 0.341072, Training Accuracy= 0.86538\n",
      "Step 8600, Minibatch Loss= 0.323062, Training Accuracy= 0.87500\n",
      "Step 8800, Minibatch Loss= 0.299278, Training Accuracy= 0.89423\n",
      "Step 9000, Minibatch Loss= 0.273857, Training Accuracy= 0.90385\n",
      "Step 9200, Minibatch Loss= 0.248392, Training Accuracy= 0.91346\n",
      "Step 9400, Minibatch Loss= 0.221348, Training Accuracy= 0.92308\n",
      "Step 9600, Minibatch Loss= 0.191947, Training Accuracy= 0.92308\n",
      "Step 9800, Minibatch Loss= 0.159308, Training Accuracy= 0.93269\n",
      "Step 10000, Minibatch Loss= 0.136938, Training Accuracy= 0.96154\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.952\n"
     ]
    }
   ],
   "source": [
    "# Iniciar entrenamiento\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    # Ejecutar el inicializador\n",
    "    sess.run(init)\n",
    "\n",
    "    for step in range(1, training_steps+1):\n",
    "        batch_x, batch_y, batch_seqlen = trainset.next(batch_size)\n",
    "        # Optimización de la ejecución (backprop)\n",
    "        sess.run(optimizer, feed_dict={x: batch_x, y: batch_y,\n",
    "                                       seqlen: batch_seqlen})\n",
    "        if step % display_step == 0 or step == 1:\n",
    "            # Calcular la precisión y la pérdida del lote\n",
    "            acc, loss = sess.run([accuracy, cost], feed_dict={x: batch_x, y: batch_y,\n",
    "                                                seqlen: batch_seqlen})\n",
    "            print(\"Step \" + str(step) + \", Minibatch Loss= \" + \\\n",
    "                  \"{:.6f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                  \"{:.5f}\".format(acc))\n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    # Calcular la precisión\n",
    "    test_data = testset.data\n",
    "    test_label = testset.labels\n",
    "    test_seqlen = testset.seqlen\n",
    "    print(\"Testing Accuracy:\", \\\n",
    "        sess.run(accuracy, feed_dict={x: test_data, y: test_label,\n",
    "                                      seqlen: test_seqlen}))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
