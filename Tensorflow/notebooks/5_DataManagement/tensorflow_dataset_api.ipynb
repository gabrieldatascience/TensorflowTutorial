{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow Dataset API\n",
    "\n",
    "En este ejemplo, mostraremos cómo cargar datos de arreglos numéricos en el nuevo archivo \n",
    "TensorFlow 'Dataset' API. El Dataset API implementa un canal de datos optimizado\n",
    "con colas, que agilizan el procesamiento de datos y la formación (especialmente en la GPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Importar datos MNIST (Numpy format)\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parametros\n",
    "learning_rate = 0.01\n",
    "num_steps = 1000\n",
    "batch_size = 128\n",
    "display_step = 100\n",
    "\n",
    "# Network Parametros\n",
    "n_input = 784 # Entrada de datos MNIST (forma de la imagen: 28*28)\n",
    "n_classes = 10 # Clases totales del MNIST (0-9 dígitos)\n",
    "dropout = 0.75 # Abandono, probabilidad de mantener las unidades\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "# Crear un tensor de conjunto de datos a partir de las imágenes y las etiquetas\n",
    "dataset = tf.contrib.data.Dataset.from_tensor_slices(\n",
    "    (mnist.train.images, mnist.train.labels))\n",
    "# Crear lotes de datos\n",
    "dataset = dataset.batch(batch_size)\n",
    "# Crear un iterador, para repasar el conjunto de datos\n",
    "iterator = dataset.make_initializable_iterator()\n",
    "# Es mejor usar 2 marcadores de posición, para evitar cargar todos los datos en la memoria,\n",
    "# y evitar la longitud de restricción de 2Gb de un tensor.\n",
    "_data = tf.placeholder(tf.float32, [None, n_input])\n",
    "_labels = tf.placeholder(tf.float32, [None, n_classes])\n",
    "# Inicializar el iterador\n",
    "sess.run(iterator.initializer, feed_dict={_data: mnist.train.images,\n",
    "                                          _labels: mnist.train.labels})\n",
    "\n",
    "# Entrada de red neuronal\n",
    "X, Y = iterator.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -----------------------------------------------\n",
    "# ESTA ES UNA CNN CLÁSICA (ver ejemplos, sección 3)\n",
    "# -----------------------------------------------\n",
    "# Tenga en cuenta que algunos elementos han cambiado (uso de sess run).\n",
    "\n",
    "# Crear modelo\n",
    "def conv_net(x, n_classes, dropout, reuse, is_training):\n",
    "    # Definir un ámbito de reutilización de las variables\n",
    "    with tf.variable_scope('ConvNet', reuse=reuse):\n",
    "        # La entrada de datos MNIST es un vector 1-D de 784 características (28*28 píxeles)\n",
    "        # Modificar para que coincida con el formato de la imagen[Altura x Anchura x Canal]\n",
    "        # La entrada del tensor se convierte en 4-D:[Tamaño del lote, altura, ancho, canal]\n",
    "        x = tf.reshape(x, shape=[-1, 28, 28, 1])\n",
    "\n",
    "        # Capa de convolución con 32 filtros y un tamaño de kernel de 5\n",
    "        conv1 = tf.layers.conv2d(x, 32, 5, activation=tf.nn.relu)\n",
    "        # Agrupación máxima (muestreo descendente) con zancadas de 2 y tamaño de kernel de 2\n",
    "        conv1 = tf.layers.max_pooling2d(conv1, 2, 2)\n",
    "\n",
    "        # Capa de convolución con 32 filtros y un tamaño de kernel de 5\n",
    "        conv2 = tf.layers.conv2d(conv1, 64, 3, activation=tf.nn.relu)\n",
    "        # Agrupación máxima (muestreo descendente) con zancadas de 2 y tamaño de kernel de 2\n",
    "        conv2 = tf.layers.max_pooling2d(conv2, 2, 2)\n",
    "\n",
    "        # Aplanar los datos a un vector 1-D para la capa completamente conectada.\n",
    "        fc1 = tf.contrib.layers.flatten(conv2)\n",
    "\n",
    "        # Capa completamente conectada (en la carpeta contrib por ahora)\n",
    "        fc1 = tf.layers.dense(fc1, 1024)\n",
    "        # Aplicar Abandono (si is_training es Falso, no se aplica el abandono)\n",
    "        fc1 = tf.layers.dropout(fc1, rate=dropout, training=is_training)\n",
    "\n",
    "        # Capa de salida, predicción de clases\n",
    "        out = tf.layers.dense(fc1, n_classes)\n",
    "        # Porque 'softmax_cross_entropy_with_logits' ya aplica softmax,\n",
    "        # sólo aplicamos softmax a la red de pruebas\n",
    "        out = tf.nn.softmax(out) if not is_training else out\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "# Debido a que los Abandonados tienen un comportamiento diferente en el entrenamiento y en el tiempo de predicción, nosotros\n",
    "# necesitan crear 2 gráficos de cálculo distintos que compartan los mismos pesos.\n",
    "\n",
    "# Cree un gráfico para el entrenamiento\n",
    "logits_train = conv_net(X, n_classes, dropout, reuse=False, is_training=True)\n",
    "# Cree otro gráfico para probar que reutilice los mismos pesos, pero que tenga\n",
    "# comportamiento diferente para la \"deserción escolar\" (no se aplica).\n",
    "logits_test = conv_net(X, n_classes, dropout, reuse=True, is_training=False)\n",
    "\n",
    "# Definir la pérdida y el optimizador (con los registros del tren, para que la pérdida de datos tenga efecto)\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "    logits=logits_train, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "# Evaluar el modelo (con registros de pruebas, para deshabilitar la deserción escolar)\n",
    "correct_pred = tf.equal(tf.argmax(logits_test, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1, Minibatch Loss= 7.9429, Training Accuracy= 0.070\n",
      "Step 100, Minibatch Loss= 0.3491, Training Accuracy= 0.922\n",
      "Step 200, Minibatch Loss= 0.2343, Training Accuracy= 0.922\n",
      "Step 300, Minibatch Loss= 0.1838, Training Accuracy= 0.969\n",
      "Step 400, Minibatch Loss= 0.1715, Training Accuracy= 0.953\n",
      "Step 500, Minibatch Loss= 0.2730, Training Accuracy= 0.938\n",
      "Step 600, Minibatch Loss= 0.3427, Training Accuracy= 0.953\n",
      "Step 700, Minibatch Loss= 0.2261, Training Accuracy= 0.961\n",
      "Step 800, Minibatch Loss= 0.1487, Training Accuracy= 0.953\n",
      "Step 900, Minibatch Loss= 0.1438, Training Accuracy= 0.945\n",
      "Step 1000, Minibatch Loss= 0.1786, Training Accuracy= 0.961\n",
      "Optimization Finished!\n"
     ]
    }
   ],
   "source": [
    "# Inicializar las variables (es decir, asignar su valor por defecto)\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Ejecutar el inicializador\n",
    "sess.run(init)\n",
    "\n",
    "# Circulo de entrenamiento\n",
    "for step in range(1, num_steps + 1):\n",
    "    \n",
    "    try:\n",
    "        # Ejecutar Optimización\n",
    "        sess.run(train_op)\n",
    "    except tf.errors.OutOfRangeError:\n",
    "        # Recargar el iterador cuando llegue al final del conjunto de datos\n",
    "        sess.run(iterator.initializer, \n",
    "                 feed_dict={_data: mnist.train.images,\n",
    "                            _labels: mnist.train.labels})\n",
    "        sess.run(train_op)\n",
    "        \n",
    "    if step % display_step == 0 or step == 1:\n",
    "        # Calcular la pérdida y la precisión de los lotes\n",
    "        # (note que esto consume un nuevo lote de datos)\n",
    "        loss, acc = sess.run([loss_op, accuracy])\n",
    "        print(\"Step \" + str(step) + \", Minibatch Loss= \" + \\\n",
    "              \"{:.4f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "              \"{:.3f}\".format(acc))\n",
    "\n",
    "print(\"Optimization Finished!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
